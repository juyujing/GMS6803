[2025-12-10 03:03:00] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 256, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 03:03:01] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 03:03:05] Start Training...
[2025-12-10 03:03:07] Epoch [1/50] Train Loss: 0.3141 | Val Loss: 0.3338 | AUC: 0.8611 | F1: 0.8722
[2025-12-10 03:03:07] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8611_lr_0.0001.pth
[2025-12-10 03:03:07] Epoch [2/50] Train Loss: 0.3097 | Val Loss: 0.3122 | AUC: 0.8676 | F1: 0.8379
[2025-12-10 03:03:07] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8676_lr_0.0001.pth
[2025-12-10 03:03:07] Epoch [3/50] Train Loss: 0.2997 | Val Loss: 0.3065 | AUC: 0.8709 | F1: 0.9000
[2025-12-10 03:03:07] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8709_lr_0.0001.pth
[2025-12-10 03:03:07] Epoch [4/50] Train Loss: 0.2975 | Val Loss: 0.2993 | AUC: 0.8715 | F1: 0.8544
[2025-12-10 03:03:07] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8715_lr_0.0001.pth
[2025-12-10 03:03:07] Epoch [5/50] Train Loss: 0.2908 | Val Loss: 0.2929 | AUC: 0.8740 | F1: 0.8431
[2025-12-10 03:03:07] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8740_lr_0.0001.pth
[2025-12-10 03:03:08] Epoch [6/50] Train Loss: 0.2794 | Val Loss: 0.2870 | AUC: 0.8747 | F1: 0.8081
[2025-12-10 03:03:08] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8747_lr_0.0001.pth
[2025-12-10 03:03:08] Epoch [7/50] Train Loss: 0.2775 | Val Loss: 0.2804 | AUC: 0.8727 | F1: 0.8431
[2025-12-10 03:03:08] Epoch [8/50] Train Loss: 0.2730 | Val Loss: 0.2734 | AUC: 0.9389 | F1: 0.8000
[2025-12-10 03:03:08] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9389_lr_0.0001.pth
[2025-12-10 03:03:08] Epoch [9/50] Train Loss: 0.2637 | Val Loss: 0.2674 | AUC: 0.9412 | F1: 0.7684
[2025-12-10 03:03:08] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9412_lr_0.0001.pth
[2025-12-10 03:03:08] Epoch [10/50] Train Loss: 0.2624 | Val Loss: 0.2618 | AUC: 0.9338 | F1: 0.8000
[2025-12-10 03:03:09] Epoch [11/50] Train Loss: 0.2469 | Val Loss: 0.2560 | AUC: 0.9323 | F1: 0.8641
[2025-12-10 03:03:09] Epoch [12/50] Train Loss: 0.2562 | Val Loss: 0.2511 | AUC: 0.9308 | F1: 0.8626
[2025-12-10 03:03:09] Epoch [13/50] Train Loss: 0.2500 | Val Loss: 0.2456 | AUC: 0.9313 | F1: 0.8358
[2025-12-10 03:03:09] Epoch [14/50] Train Loss: 0.2476 | Val Loss: 0.2413 | AUC: 0.9320 | F1: 0.8585
[2025-12-10 03:03:09] Epoch [15/50] Train Loss: 0.2443 | Val Loss: 0.2393 | AUC: 0.9184 | F1: 0.8000
[2025-12-10 03:03:09] Epoch [16/50] Train Loss: 0.2395 | Val Loss: 0.2335 | AUC: 0.9442 | F1: 0.8000
[2025-12-10 03:03:09] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9442_lr_0.0001.pth
[2025-12-10 03:03:09] Epoch [17/50] Train Loss: 0.2356 | Val Loss: 0.2297 | AUC: 0.9288 | F1: 0.8350
[2025-12-10 03:03:09] Epoch [18/50] Train Loss: 0.2277 | Val Loss: 0.2237 | AUC: 0.9462 | F1: 0.8000
[2025-12-10 03:03:09] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9462_lr_0.0001.pth
[2025-12-10 03:03:09] Epoch [19/50] Train Loss: 0.2283 | Val Loss: 0.2226 | AUC: 0.9470 | F1: 0.8241
[2025-12-10 03:03:09] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9470_lr_0.0001.pth
[2025-12-10 03:03:10] Epoch [20/50] Train Loss: 0.2287 | Val Loss: 0.2153 | AUC: 0.9384 | F1: 0.8529
[2025-12-10 03:03:10] Epoch [21/50] Train Loss: 0.2282 | Val Loss: 0.2160 | AUC: 0.9417 | F1: 0.8000
[2025-12-10 03:03:10] Epoch [22/50] Train Loss: 0.2212 | Val Loss: 0.2112 | AUC: 0.9452 | F1: 0.8000
[2025-12-10 03:03:10] Epoch [23/50] Train Loss: 0.2130 | Val Loss: 0.2107 | AUC: 0.9460 | F1: 0.8241
[2025-12-10 03:03:10] Epoch [24/50] Train Loss: 0.2088 | Val Loss: 0.2061 | AUC: 0.9285 | F1: 0.8462
[2025-12-10 03:03:10] Epoch [25/50] Train Loss: 0.2187 | Val Loss: 0.2038 | AUC: 0.9397 | F1: 0.8473
[2025-12-10 03:03:10] Epoch [26/50] Train Loss: 0.2155 | Val Loss: 0.2040 | AUC: 0.9493 | F1: 0.8241
[2025-12-10 03:03:10] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9493_lr_0.0001.pth
[2025-12-10 03:03:10] Epoch [27/50] Train Loss: 0.2182 | Val Loss: 0.1995 | AUC: 0.9465 | F1: 0.8000
[2025-12-10 03:03:10] Epoch [28/50] Train Loss: 0.2170 | Val Loss: 0.1948 | AUC: 0.9488 | F1: 0.8241
[2025-12-10 03:03:10] Epoch [29/50] Train Loss: 0.2225 | Val Loss: 0.1952 | AUC: 0.9437 | F1: 0.8529
[2025-12-10 03:03:10] Epoch [30/50] Train Loss: 0.2162 | Val Loss: 0.1922 | AUC: 0.9402 | F1: 0.8750
[2025-12-10 03:03:10] Epoch [31/50] Train Loss: 0.2086 | Val Loss: 0.1920 | AUC: 0.9452 | F1: 0.8182
[2025-12-10 03:03:10] Epoch [32/50] Train Loss: 0.2030 | Val Loss: 0.1899 | AUC: 0.9336 | F1: 0.8679
[2025-12-10 03:03:10] Epoch [33/50] Train Loss: 0.2065 | Val Loss: 0.1871 | AUC: 0.9445 | F1: 0.8529
[2025-12-10 03:03:11] Epoch [34/50] Train Loss: 0.2073 | Val Loss: 0.1845 | AUC: 0.9447 | F1: 0.8529
[2025-12-10 03:03:11] Epoch [35/50] Train Loss: 0.2014 | Val Loss: 0.1861 | AUC: 0.9427 | F1: 0.8585
[2025-12-10 03:03:11] Epoch [36/50] Train Loss: 0.1975 | Val Loss: 0.1826 | AUC: 0.9457 | F1: 0.8473
[2025-12-10 03:03:11] Epoch [37/50] Train Loss: 0.2097 | Val Loss: 0.1862 | AUC: 0.9386 | F1: 0.8416
[2025-12-10 03:03:11] Epoch [38/50] Train Loss: 0.1984 | Val Loss: 0.1813 | AUC: 0.9427 | F1: 0.8750
[2025-12-10 03:03:11] Epoch [39/50] Train Loss: 0.2080 | Val Loss: 0.1814 | AUC: 0.9498 | F1: 0.8241
[2025-12-10 03:03:11] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9498_lr_0.0001.pth
[2025-12-10 03:03:11] Epoch [40/50] Train Loss: 0.1960 | Val Loss: 0.1770 | AUC: 0.9478 | F1: 0.8585
[2025-12-10 03:03:11] Epoch [41/50] Train Loss: 0.2070 | Val Loss: 0.1773 | AUC: 0.9495 | F1: 0.8358
[2025-12-10 03:03:11] Epoch [42/50] Train Loss: 0.2028 | Val Loss: 0.1750 | AUC: 0.9468 | F1: 0.8529
[2025-12-10 03:03:12] Epoch [43/50] Train Loss: 0.1971 | Val Loss: 0.1759 | AUC: 0.9435 | F1: 0.8585
[2025-12-10 03:03:12] Epoch [44/50] Train Loss: 0.2023 | Val Loss: 0.1799 | AUC: 0.9457 | F1: 0.8241
[2025-12-10 03:03:12] Epoch [45/50] Train Loss: 0.2033 | Val Loss: 0.1709 | AUC: 0.9480 | F1: 0.8696
[2025-12-10 03:03:12] Epoch [46/50] Train Loss: 0.1973 | Val Loss: 0.1797 | AUC: 0.9384 | F1: 0.8416
[2025-12-10 03:03:12] Epoch [47/50] Train Loss: 0.1952 | Val Loss: 0.1693 | AUC: 0.9485 | F1: 0.8641
[2025-12-10 03:03:12] Epoch [48/50] Train Loss: 0.2005 | Val Loss: 0.1725 | AUC: 0.9495 | F1: 0.8416
[2025-12-10 03:03:13] Epoch [49/50] Train Loss: 0.1993 | Val Loss: 0.1696 | AUC: 0.9455 | F1: 0.8696
[2025-12-10 03:03:13] Epoch [50/50] Train Loss: 0.1997 | Val Loss: 0.1688 | AUC: 0.9440 | F1: 0.8857
[2025-12-10 03:03:13] Training Finished. Running Test on Best Model...
[2025-12-10 03:03:13] Test Results - AUC: 0.9026 | F1: 0.8416
[2025-12-10 03:03:37] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 512, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 03:03:39] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 03:03:41] Start Training...
[2025-12-10 03:03:44] Epoch [1/50] Train Loss: 0.3053 | Val Loss: 0.3084 | AUC: 0.8712 | F1: 0.8722
[2025-12-10 03:03:44] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8712_lr_0.0001.pth
[2025-12-10 03:03:45] Epoch [2/50] Train Loss: 0.2915 | Val Loss: 0.2856 | AUC: 0.8981 | F1: 0.9099
[2025-12-10 03:03:45] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8981_lr_0.0001.pth
[2025-12-10 03:03:45] Epoch [3/50] Train Loss: 0.2812 | Val Loss: 0.2703 | AUC: 0.9151 | F1: 0.8300
[2025-12-10 03:03:45] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9151_lr_0.0001.pth
[2025-12-10 03:03:45] Epoch [4/50] Train Loss: 0.2598 | Val Loss: 0.2556 | AUC: 0.9384 | F1: 0.8061
[2025-12-10 03:03:45] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9384_lr_0.0001.pth
[2025-12-10 03:03:45] Epoch [5/50] Train Loss: 0.2418 | Val Loss: 0.2458 | AUC: 0.9303 | F1: 0.8571
[2025-12-10 03:03:45] Epoch [6/50] Train Loss: 0.2429 | Val Loss: 0.2385 | AUC: 0.9450 | F1: 0.8473
[2025-12-10 03:03:45] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9450_lr_0.0001.pth
[2025-12-10 03:03:45] Epoch [7/50] Train Loss: 0.2384 | Val Loss: 0.2295 | AUC: 0.9389 | F1: 0.8300
[2025-12-10 03:03:45] Epoch [8/50] Train Loss: 0.2301 | Val Loss: 0.2217 | AUC: 0.9194 | F1: 0.8837
[2025-12-10 03:03:45] Epoch [9/50] Train Loss: 0.2304 | Val Loss: 0.2137 | AUC: 0.9457 | F1: 0.8061
[2025-12-10 03:03:45] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9457_lr_0.0001.pth
[2025-12-10 03:03:46] Epoch [10/50] Train Loss: 0.2303 | Val Loss: 0.2111 | AUC: 0.9148 | F1: 0.8517
[2025-12-10 03:03:46] Epoch [11/50] Train Loss: 0.2132 | Val Loss: 0.2016 | AUC: 0.9305 | F1: 0.8732
[2025-12-10 03:03:46] Epoch [12/50] Train Loss: 0.2200 | Val Loss: 0.1975 | AUC: 0.9366 | F1: 0.8502
[2025-12-10 03:03:46] Epoch [13/50] Train Loss: 0.2165 | Val Loss: 0.1915 | AUC: 0.9402 | F1: 0.8785
[2025-12-10 03:03:46] Epoch [14/50] Train Loss: 0.2045 | Val Loss: 0.1890 | AUC: 0.9475 | F1: 0.8696
[2025-12-10 03:03:46] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9475_lr_0.0001.pth
[2025-12-10 03:03:46] Epoch [15/50] Train Loss: 0.2135 | Val Loss: 0.1845 | AUC: 0.9427 | F1: 0.8641
[2025-12-10 03:03:46] Epoch [16/50] Train Loss: 0.2129 | Val Loss: 0.1847 | AUC: 0.9526 | F1: 0.8241
[2025-12-10 03:03:46] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9526_lr_0.0001.pth
[2025-12-10 03:03:46] Epoch [17/50] Train Loss: 0.1934 | Val Loss: 0.1763 | AUC: 0.9424 | F1: 0.9041
[2025-12-10 03:03:46] Epoch [18/50] Train Loss: 0.2043 | Val Loss: 0.1747 | AUC: 0.9528 | F1: 0.8416
[2025-12-10 03:03:46] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9528_lr_0.0001.pth
[2025-12-10 03:03:47] Epoch [19/50] Train Loss: 0.1989 | Val Loss: 0.1734 | AUC: 0.9412 | F1: 0.8732
[2025-12-10 03:03:47] Epoch [20/50] Train Loss: 0.1941 | Val Loss: 0.1734 | AUC: 0.9493 | F1: 0.8962
[2025-12-10 03:03:48] Epoch [21/50] Train Loss: 0.1953 | Val Loss: 0.1698 | AUC: 0.9409 | F1: 0.8940
[2025-12-10 03:03:48] Epoch [22/50] Train Loss: 0.2030 | Val Loss: 0.1676 | AUC: 0.9508 | F1: 0.8696
[2025-12-10 03:03:48] Epoch [23/50] Train Loss: 0.2121 | Val Loss: 0.1667 | AUC: 0.9533 | F1: 0.8416
[2025-12-10 03:03:48] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9533_lr_0.0001.pth
[2025-12-10 03:03:48] Epoch [24/50] Train Loss: 0.2043 | Val Loss: 0.1660 | AUC: 0.9435 | F1: 0.8826
[2025-12-10 03:03:48] Epoch [25/50] Train Loss: 0.2022 | Val Loss: 0.1656 | AUC: 0.9488 | F1: 0.8750
[2025-12-10 03:03:48] Epoch [26/50] Train Loss: 0.2029 | Val Loss: 0.1684 | AUC: 0.9460 | F1: 0.8641
[2025-12-10 03:03:48] Epoch [27/50] Train Loss: 0.1979 | Val Loss: 0.1655 | AUC: 0.9493 | F1: 0.8696
[2025-12-10 03:03:48] Epoch [28/50] Train Loss: 0.1982 | Val Loss: 0.1651 | AUC: 0.9506 | F1: 0.8529
[2025-12-10 03:03:48] Epoch [29/50] Train Loss: 0.2020 | Val Loss: 0.1613 | AUC: 0.9508 | F1: 0.8857
[2025-12-10 03:03:48] Epoch [30/50] Train Loss: 0.2001 | Val Loss: 0.1644 | AUC: 0.9495 | F1: 0.8641
[2025-12-10 03:03:48] Epoch [31/50] Train Loss: 0.1982 | Val Loss: 0.1632 | AUC: 0.9518 | F1: 0.8529
[2025-12-10 03:03:48] Epoch [32/50] Train Loss: 0.1914 | Val Loss: 0.1622 | AUC: 0.9460 | F1: 0.8708
[2025-12-10 03:03:49] Epoch [33/50] Train Loss: 0.2014 | Val Loss: 0.1585 | AUC: 0.9501 | F1: 0.9124
[2025-12-10 03:03:49] Epoch [34/50] Train Loss: 0.1938 | Val Loss: 0.1626 | AUC: 0.9506 | F1: 0.8585
[2025-12-10 03:03:49] Epoch [35/50] Train Loss: 0.2019 | Val Loss: 0.1664 | AUC: 0.9417 | F1: 0.8667
[2025-12-10 03:03:49] Epoch [36/50] Train Loss: 0.1954 | Val Loss: 0.1593 | AUC: 0.9546 | F1: 0.8529
[2025-12-10 03:03:49] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9546_lr_0.0001.pth
[2025-12-10 03:03:49] Epoch [37/50] Train Loss: 0.1888 | Val Loss: 0.1636 | AUC: 0.9511 | F1: 0.8529
[2025-12-10 03:03:49] Epoch [38/50] Train Loss: 0.1921 | Val Loss: 0.1564 | AUC: 0.9513 | F1: 0.9065
[2025-12-10 03:03:49] Epoch [39/50] Train Loss: 0.1909 | Val Loss: 0.1596 | AUC: 0.9488 | F1: 0.8641
[2025-12-10 03:03:49] Epoch [40/50] Train Loss: 0.1937 | Val Loss: 0.1542 | AUC: 0.9544 | F1: 0.9014
[2025-12-10 03:03:49] Epoch [41/50] Train Loss: 0.1924 | Val Loss: 0.1558 | AUC: 0.9518 | F1: 0.8857
[2025-12-10 03:03:49] Epoch [42/50] Train Loss: 0.1858 | Val Loss: 0.1562 | AUC: 0.9523 | F1: 0.8641
[2025-12-10 03:03:49] Epoch [43/50] Train Loss: 0.1909 | Val Loss: 0.1526 | AUC: 0.9531 | F1: 0.8962
[2025-12-10 03:03:49] Epoch [44/50] Train Loss: 0.1959 | Val Loss: 0.1521 | AUC: 0.9526 | F1: 0.9014
[2025-12-10 03:03:49] Epoch [45/50] Train Loss: 0.1902 | Val Loss: 0.1520 | AUC: 0.9518 | F1: 0.9014
[2025-12-10 03:03:49] Epoch [46/50] Train Loss: 0.1982 | Val Loss: 0.1540 | AUC: 0.9528 | F1: 0.8641
[2025-12-10 03:03:50] Epoch [47/50] Train Loss: 0.1903 | Val Loss: 0.1497 | AUC: 0.9539 | F1: 0.8962
[2025-12-10 03:03:50] Epoch [48/50] Train Loss: 0.1795 | Val Loss: 0.1528 | AUC: 0.9452 | F1: 0.8774
[2025-12-10 03:03:50] Epoch [49/50] Train Loss: 0.2018 | Val Loss: 0.1525 | AUC: 0.9506 | F1: 0.8857
[2025-12-10 03:03:50] Epoch [50/50] Train Loss: 0.1807 | Val Loss: 0.1531 | AUC: 0.9452 | F1: 0.8868
[2025-12-10 03:03:50] Training Finished. Running Test on Best Model...
[2025-12-10 03:03:50] Test Results - AUC: 0.9207 | F1: 0.8333
