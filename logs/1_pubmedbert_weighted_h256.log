[2025-12-10 01:56:51] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 256, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 01:56:53] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 01:56:56] Start Training...
[2025-12-10 01:57:03] Epoch [1/50] Train Loss: 0.3141 | Val Loss: 0.3338 | AUC: 0.8611 | F1: 0.8722
[2025-12-10 01:57:03] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8611_lr_0.0001.pth
[2025-12-10 01:57:03] Epoch [2/50] Train Loss: 0.3097 | Val Loss: 0.3122 | AUC: 0.8676 | F1: 0.8379
[2025-12-10 01:57:03] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8676_lr_0.0001.pth
[2025-12-10 01:57:03] Epoch [3/50] Train Loss: 0.2997 | Val Loss: 0.3065 | AUC: 0.8709 | F1: 0.9000
[2025-12-10 01:57:03] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8709_lr_0.0001.pth
[2025-12-10 01:57:03] Epoch [4/50] Train Loss: 0.2975 | Val Loss: 0.2993 | AUC: 0.8715 | F1: 0.8544
[2025-12-10 01:57:03] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8715_lr_0.0001.pth
[2025-12-10 01:57:04] Epoch [5/50] Train Loss: 0.2908 | Val Loss: 0.2929 | AUC: 0.8740 | F1: 0.8431
[2025-12-10 01:57:04] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8740_lr_0.0001.pth
[2025-12-10 01:57:04] Epoch [6/50] Train Loss: 0.2794 | Val Loss: 0.2870 | AUC: 0.8747 | F1: 0.8081
[2025-12-10 01:57:04] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8747_lr_0.0001.pth
[2025-12-10 01:57:04] Epoch [7/50] Train Loss: 0.2775 | Val Loss: 0.2804 | AUC: 0.8727 | F1: 0.8431
[2025-12-10 01:57:04] Epoch [8/50] Train Loss: 0.2730 | Val Loss: 0.2734 | AUC: 0.9389 | F1: 0.8000
[2025-12-10 01:57:04] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9389_lr_0.0001.pth
[2025-12-10 01:57:04] Epoch [9/50] Train Loss: 0.2637 | Val Loss: 0.2674 | AUC: 0.9412 | F1: 0.7684
[2025-12-10 01:57:04] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9412_lr_0.0001.pth
[2025-12-10 01:57:04] Epoch [10/50] Train Loss: 0.2624 | Val Loss: 0.2618 | AUC: 0.9338 | F1: 0.8000
[2025-12-10 01:57:04] Epoch [11/50] Train Loss: 0.2469 | Val Loss: 0.2560 | AUC: 0.9323 | F1: 0.8641
[2025-12-10 01:57:04] Epoch [12/50] Train Loss: 0.2562 | Val Loss: 0.2511 | AUC: 0.9308 | F1: 0.8626
[2025-12-10 01:57:04] Epoch [13/50] Train Loss: 0.2500 | Val Loss: 0.2456 | AUC: 0.9313 | F1: 0.8358
[2025-12-10 01:57:05] Epoch [14/50] Train Loss: 0.2476 | Val Loss: 0.2413 | AUC: 0.9320 | F1: 0.8585
[2025-12-10 01:57:05] Epoch [15/50] Train Loss: 0.2443 | Val Loss: 0.2393 | AUC: 0.9184 | F1: 0.8000
[2025-12-10 01:57:05] Epoch [16/50] Train Loss: 0.2395 | Val Loss: 0.2335 | AUC: 0.9442 | F1: 0.8000
[2025-12-10 01:57:05] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9442_lr_0.0001.pth
[2025-12-10 01:57:05] Epoch [17/50] Train Loss: 0.2356 | Val Loss: 0.2297 | AUC: 0.9288 | F1: 0.8350
[2025-12-10 01:57:05] Epoch [18/50] Train Loss: 0.2277 | Val Loss: 0.2237 | AUC: 0.9462 | F1: 0.8000
[2025-12-10 01:57:05] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9462_lr_0.0001.pth
[2025-12-10 01:57:05] Epoch [19/50] Train Loss: 0.2283 | Val Loss: 0.2226 | AUC: 0.9470 | F1: 0.8241
[2025-12-10 01:57:05] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9470_lr_0.0001.pth
[2025-12-10 01:57:05] Epoch [20/50] Train Loss: 0.2287 | Val Loss: 0.2153 | AUC: 0.9384 | F1: 0.8529
[2025-12-10 01:57:05] Epoch [21/50] Train Loss: 0.2282 | Val Loss: 0.2160 | AUC: 0.9417 | F1: 0.8000
[2025-12-10 01:57:05] Epoch [22/50] Train Loss: 0.2212 | Val Loss: 0.2112 | AUC: 0.9452 | F1: 0.8000
[2025-12-10 01:57:05] Epoch [23/50] Train Loss: 0.2130 | Val Loss: 0.2107 | AUC: 0.9460 | F1: 0.8241
[2025-12-10 01:57:05] Epoch [24/50] Train Loss: 0.2088 | Val Loss: 0.2061 | AUC: 0.9285 | F1: 0.8462
[2025-12-10 01:57:05] Epoch [25/50] Train Loss: 0.2187 | Val Loss: 0.2038 | AUC: 0.9397 | F1: 0.8473
[2025-12-10 01:57:06] Epoch [26/50] Train Loss: 0.2155 | Val Loss: 0.2040 | AUC: 0.9493 | F1: 0.8241
[2025-12-10 01:57:06] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9493_lr_0.0001.pth
[2025-12-10 01:57:06] Epoch [27/50] Train Loss: 0.2182 | Val Loss: 0.1995 | AUC: 0.9465 | F1: 0.8000
[2025-12-10 01:57:06] Epoch [28/50] Train Loss: 0.2170 | Val Loss: 0.1948 | AUC: 0.9488 | F1: 0.8241
[2025-12-10 01:57:07] Epoch [29/50] Train Loss: 0.2225 | Val Loss: 0.1952 | AUC: 0.9437 | F1: 0.8529
[2025-12-10 01:57:07] Epoch [30/50] Train Loss: 0.2162 | Val Loss: 0.1922 | AUC: 0.9402 | F1: 0.8750
[2025-12-10 01:57:07] Epoch [31/50] Train Loss: 0.2086 | Val Loss: 0.1920 | AUC: 0.9452 | F1: 0.8182
[2025-12-10 01:57:07] Epoch [32/50] Train Loss: 0.2030 | Val Loss: 0.1899 | AUC: 0.9336 | F1: 0.8679
[2025-12-10 01:57:07] Epoch [33/50] Train Loss: 0.2065 | Val Loss: 0.1871 | AUC: 0.9445 | F1: 0.8529
[2025-12-10 01:57:07] Epoch [34/50] Train Loss: 0.2073 | Val Loss: 0.1845 | AUC: 0.9447 | F1: 0.8529
[2025-12-10 01:57:08] Epoch [35/50] Train Loss: 0.2014 | Val Loss: 0.1861 | AUC: 0.9427 | F1: 0.8585
[2025-12-10 01:57:08] Epoch [36/50] Train Loss: 0.1975 | Val Loss: 0.1826 | AUC: 0.9457 | F1: 0.8473
[2025-12-10 01:57:08] Epoch [37/50] Train Loss: 0.2097 | Val Loss: 0.1862 | AUC: 0.9386 | F1: 0.8416
[2025-12-10 01:57:08] Epoch [38/50] Train Loss: 0.1984 | Val Loss: 0.1813 | AUC: 0.9427 | F1: 0.8750
[2025-12-10 01:57:08] Epoch [39/50] Train Loss: 0.2080 | Val Loss: 0.1814 | AUC: 0.9498 | F1: 0.8241
[2025-12-10 01:57:08] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9498_lr_0.0001.pth
[2025-12-10 01:57:08] Epoch [40/50] Train Loss: 0.1960 | Val Loss: 0.1770 | AUC: 0.9478 | F1: 0.8585
[2025-12-10 01:57:08] Epoch [41/50] Train Loss: 0.2070 | Val Loss: 0.1773 | AUC: 0.9495 | F1: 0.8358
[2025-12-10 01:57:08] Epoch [42/50] Train Loss: 0.2028 | Val Loss: 0.1750 | AUC: 0.9468 | F1: 0.8529
[2025-12-10 01:57:08] Epoch [43/50] Train Loss: 0.1971 | Val Loss: 0.1759 | AUC: 0.9435 | F1: 0.8585
[2025-12-10 01:57:08] Epoch [44/50] Train Loss: 0.2023 | Val Loss: 0.1799 | AUC: 0.9457 | F1: 0.8241
[2025-12-10 01:57:08] Epoch [45/50] Train Loss: 0.2033 | Val Loss: 0.1709 | AUC: 0.9480 | F1: 0.8696
[2025-12-10 01:57:08] Epoch [46/50] Train Loss: 0.1973 | Val Loss: 0.1797 | AUC: 0.9384 | F1: 0.8416
[2025-12-10 01:57:09] Epoch [47/50] Train Loss: 0.1952 | Val Loss: 0.1693 | AUC: 0.9485 | F1: 0.8641
[2025-12-10 01:57:09] Epoch [48/50] Train Loss: 0.2005 | Val Loss: 0.1725 | AUC: 0.9495 | F1: 0.8416
[2025-12-10 01:57:09] Epoch [49/50] Train Loss: 0.1993 | Val Loss: 0.1696 | AUC: 0.9455 | F1: 0.8696
[2025-12-10 01:57:09] Epoch [50/50] Train Loss: 0.1997 | Val Loss: 0.1688 | AUC: 0.9440 | F1: 0.8857
[2025-12-10 01:57:09] Training Finished. Running Test on Best Model...
[2025-12-10 01:57:09] Test Results - AUC: 0.9026 | F1: 0.8416
[2025-12-10 02:11:24] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 256, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 02:11:26] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 02:11:28] Start Training...
[2025-12-10 02:12:18] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 256, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 02:12:19] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 02:12:22] Start Training...
[2025-12-10 02:12:28] Epoch [1/50] Train Loss: 0.3141 | Val Loss: 0.3338 | AUC: 0.8611 | F1: 0.8722
[2025-12-10 02:12:28] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8611_lr_0.0001.pth
[2025-12-10 02:12:28] Epoch [2/50] Train Loss: 0.3097 | Val Loss: 0.3122 | AUC: 0.8676 | F1: 0.8379
[2025-12-10 02:12:28] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8676_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [3/50] Train Loss: 0.2997 | Val Loss: 0.3065 | AUC: 0.8709 | F1: 0.9000
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8709_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [4/50] Train Loss: 0.2975 | Val Loss: 0.2993 | AUC: 0.8715 | F1: 0.8544
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8715_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [5/50] Train Loss: 0.2908 | Val Loss: 0.2929 | AUC: 0.8740 | F1: 0.8431
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8740_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [6/50] Train Loss: 0.2794 | Val Loss: 0.2870 | AUC: 0.8747 | F1: 0.8081
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8747_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [7/50] Train Loss: 0.2775 | Val Loss: 0.2804 | AUC: 0.8727 | F1: 0.8431
[2025-12-10 02:12:29] Epoch [8/50] Train Loss: 0.2730 | Val Loss: 0.2734 | AUC: 0.9389 | F1: 0.8000
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9389_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [9/50] Train Loss: 0.2637 | Val Loss: 0.2674 | AUC: 0.9412 | F1: 0.7684
[2025-12-10 02:12:29] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9412_lr_0.0001.pth
[2025-12-10 02:12:29] Epoch [10/50] Train Loss: 0.2624 | Val Loss: 0.2618 | AUC: 0.9338 | F1: 0.8000
[2025-12-10 02:12:29] Epoch [11/50] Train Loss: 0.2469 | Val Loss: 0.2560 | AUC: 0.9323 | F1: 0.8641
[2025-12-10 02:12:29] Epoch [12/50] Train Loss: 0.2562 | Val Loss: 0.2511 | AUC: 0.9308 | F1: 0.8626
[2025-12-10 02:12:29] Epoch [13/50] Train Loss: 0.2500 | Val Loss: 0.2456 | AUC: 0.9313 | F1: 0.8358
[2025-12-10 02:12:29] Epoch [14/50] Train Loss: 0.2476 | Val Loss: 0.2413 | AUC: 0.9320 | F1: 0.8585
[2025-12-10 02:12:29] Epoch [15/50] Train Loss: 0.2443 | Val Loss: 0.2393 | AUC: 0.9184 | F1: 0.8000
[2025-12-10 02:12:30] Epoch [16/50] Train Loss: 0.2395 | Val Loss: 0.2335 | AUC: 0.9442 | F1: 0.8000
[2025-12-10 02:12:30] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9442_lr_0.0001.pth
[2025-12-10 02:12:30] Epoch [17/50] Train Loss: 0.2356 | Val Loss: 0.2297 | AUC: 0.9288 | F1: 0.8350
[2025-12-10 02:12:30] Epoch [18/50] Train Loss: 0.2277 | Val Loss: 0.2237 | AUC: 0.9462 | F1: 0.8000
[2025-12-10 02:12:30] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9462_lr_0.0001.pth
[2025-12-10 02:12:30] Epoch [19/50] Train Loss: 0.2283 | Val Loss: 0.2226 | AUC: 0.9470 | F1: 0.8241
[2025-12-10 02:12:30] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9470_lr_0.0001.pth
[2025-12-10 02:12:30] Epoch [20/50] Train Loss: 0.2287 | Val Loss: 0.2153 | AUC: 0.9384 | F1: 0.8529
[2025-12-10 02:12:30] Epoch [21/50] Train Loss: 0.2282 | Val Loss: 0.2160 | AUC: 0.9417 | F1: 0.8000
[2025-12-10 02:12:30] Epoch [22/50] Train Loss: 0.2212 | Val Loss: 0.2112 | AUC: 0.9452 | F1: 0.8000
[2025-12-10 02:12:30] Epoch [23/50] Train Loss: 0.2130 | Val Loss: 0.2107 | AUC: 0.9460 | F1: 0.8241
[2025-12-10 02:12:30] Epoch [24/50] Train Loss: 0.2088 | Val Loss: 0.2061 | AUC: 0.9285 | F1: 0.8462
[2025-12-10 02:12:30] Epoch [25/50] Train Loss: 0.2187 | Val Loss: 0.2038 | AUC: 0.9397 | F1: 0.8473
[2025-12-10 02:12:30] Epoch [26/50] Train Loss: 0.2155 | Val Loss: 0.2040 | AUC: 0.9493 | F1: 0.8241
[2025-12-10 02:12:30] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9493_lr_0.0001.pth
[2025-12-10 02:12:30] Epoch [27/50] Train Loss: 0.2182 | Val Loss: 0.1995 | AUC: 0.9465 | F1: 0.8000
[2025-12-10 02:12:30] Epoch [28/50] Train Loss: 0.2170 | Val Loss: 0.1948 | AUC: 0.9488 | F1: 0.8241
[2025-12-10 02:12:31] Epoch [29/50] Train Loss: 0.2225 | Val Loss: 0.1952 | AUC: 0.9437 | F1: 0.8529
[2025-12-10 02:12:31] Epoch [30/50] Train Loss: 0.2162 | Val Loss: 0.1922 | AUC: 0.9402 | F1: 0.8750
[2025-12-10 02:12:31] Epoch [31/50] Train Loss: 0.2086 | Val Loss: 0.1920 | AUC: 0.9452 | F1: 0.8182
[2025-12-10 02:12:31] Epoch [32/50] Train Loss: 0.2030 | Val Loss: 0.1899 | AUC: 0.9336 | F1: 0.8679
[2025-12-10 02:12:31] Epoch [33/50] Train Loss: 0.2065 | Val Loss: 0.1871 | AUC: 0.9445 | F1: 0.8529
[2025-12-10 02:12:31] Epoch [34/50] Train Loss: 0.2073 | Val Loss: 0.1845 | AUC: 0.9447 | F1: 0.8529
[2025-12-10 02:12:31] Epoch [35/50] Train Loss: 0.2014 | Val Loss: 0.1861 | AUC: 0.9427 | F1: 0.8585
[2025-12-10 02:12:31] Epoch [36/50] Train Loss: 0.1975 | Val Loss: 0.1826 | AUC: 0.9457 | F1: 0.8473
[2025-12-10 02:12:31] Epoch [37/50] Train Loss: 0.2097 | Val Loss: 0.1862 | AUC: 0.9386 | F1: 0.8416
[2025-12-10 02:12:31] Epoch [38/50] Train Loss: 0.1984 | Val Loss: 0.1813 | AUC: 0.9427 | F1: 0.8750
[2025-12-10 02:12:31] Epoch [39/50] Train Loss: 0.2080 | Val Loss: 0.1814 | AUC: 0.9498 | F1: 0.8241
[2025-12-10 02:12:31] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9498_lr_0.0001.pth
[2025-12-10 02:12:31] Epoch [40/50] Train Loss: 0.1960 | Val Loss: 0.1770 | AUC: 0.9478 | F1: 0.8585
[2025-12-10 02:12:31] Epoch [41/50] Train Loss: 0.2070 | Val Loss: 0.1773 | AUC: 0.9495 | F1: 0.8358
[2025-12-10 02:12:31] Epoch [42/50] Train Loss: 0.2028 | Val Loss: 0.1750 | AUC: 0.9468 | F1: 0.8529
[2025-12-10 02:12:31] Epoch [43/50] Train Loss: 0.1971 | Val Loss: 0.1759 | AUC: 0.9435 | F1: 0.8585
[2025-12-10 02:12:32] Epoch [44/50] Train Loss: 0.2023 | Val Loss: 0.1799 | AUC: 0.9457 | F1: 0.8241
[2025-12-10 02:12:32] Epoch [45/50] Train Loss: 0.2033 | Val Loss: 0.1709 | AUC: 0.9480 | F1: 0.8696
[2025-12-10 02:12:32] Epoch [46/50] Train Loss: 0.1973 | Val Loss: 0.1797 | AUC: 0.9384 | F1: 0.8416
[2025-12-10 02:12:32] Epoch [47/50] Train Loss: 0.1952 | Val Loss: 0.1693 | AUC: 0.9485 | F1: 0.8641
[2025-12-10 02:12:32] Epoch [48/50] Train Loss: 0.2005 | Val Loss: 0.1725 | AUC: 0.9495 | F1: 0.8416
[2025-12-10 02:12:32] Epoch [49/50] Train Loss: 0.1993 | Val Loss: 0.1696 | AUC: 0.9455 | F1: 0.8696
[2025-12-10 02:12:32] Epoch [50/50] Train Loss: 0.1997 | Val Loss: 0.1688 | AUC: 0.9440 | F1: 0.8857
[2025-12-10 02:12:32] Training Finished. Running Test on Best Model...
[2025-12-10 02:12:32] Test Results - AUC: 0.9026 | F1: 0.8416
[2025-12-10 02:15:17] Arguments: {'model_type': 'llm_mlp', 'emb_path': 'dataset/feature_embeddings.npy', 'freeze_emb': False, 'lr': 0.0001, 'batch_size': 32, 'epochs': 50, 'hidden_dim': 256, 'dropout': 0.2, 'seed': 42, 'device': 'cuda', 'exp_name': 'pubmedbert_weighted_h256', 'num_workers': 0}
[2025-12-10 02:15:19] Initializing LLM Enhanced MLP with embeddings from dataset/feature_embeddings.npy...
[2025-12-10 02:15:22] Start Training...
[2025-12-10 02:15:24] Epoch [1/50] Train Loss: 0.3141 | Val Loss: 0.3338 | AUC: 0.8611 | F1: 0.8722
[2025-12-10 02:15:24] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8611_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [2/50] Train Loss: 0.3097 | Val Loss: 0.3122 | AUC: 0.8676 | F1: 0.8379
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8676_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [3/50] Train Loss: 0.2997 | Val Loss: 0.3065 | AUC: 0.8709 | F1: 0.9000
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8709_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [4/50] Train Loss: 0.2975 | Val Loss: 0.2993 | AUC: 0.8715 | F1: 0.8544
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8715_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [5/50] Train Loss: 0.2908 | Val Loss: 0.2929 | AUC: 0.8740 | F1: 0.8431
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8740_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [6/50] Train Loss: 0.2794 | Val Loss: 0.2870 | AUC: 0.8747 | F1: 0.8081
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.8747_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [7/50] Train Loss: 0.2775 | Val Loss: 0.2804 | AUC: 0.8727 | F1: 0.8431
[2025-12-10 02:15:25] Epoch [8/50] Train Loss: 0.2730 | Val Loss: 0.2734 | AUC: 0.9389 | F1: 0.8000
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9389_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [9/50] Train Loss: 0.2637 | Val Loss: 0.2674 | AUC: 0.9412 | F1: 0.7684
[2025-12-10 02:15:25] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9412_lr_0.0001.pth
[2025-12-10 02:15:25] Epoch [10/50] Train Loss: 0.2624 | Val Loss: 0.2618 | AUC: 0.9338 | F1: 0.8000
[2025-12-10 02:15:25] Epoch [11/50] Train Loss: 0.2469 | Val Loss: 0.2560 | AUC: 0.9323 | F1: 0.8641
[2025-12-10 02:15:25] Epoch [12/50] Train Loss: 0.2562 | Val Loss: 0.2511 | AUC: 0.9308 | F1: 0.8626
[2025-12-10 02:15:25] Epoch [13/50] Train Loss: 0.2500 | Val Loss: 0.2456 | AUC: 0.9313 | F1: 0.8358
[2025-12-10 02:15:26] Epoch [14/50] Train Loss: 0.2476 | Val Loss: 0.2413 | AUC: 0.9320 | F1: 0.8585
[2025-12-10 02:15:26] Epoch [15/50] Train Loss: 0.2443 | Val Loss: 0.2393 | AUC: 0.9184 | F1: 0.8000
[2025-12-10 02:15:26] Epoch [16/50] Train Loss: 0.2395 | Val Loss: 0.2335 | AUC: 0.9442 | F1: 0.8000
[2025-12-10 02:15:26] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9442_lr_0.0001.pth
[2025-12-10 02:15:26] Epoch [17/50] Train Loss: 0.2356 | Val Loss: 0.2297 | AUC: 0.9288 | F1: 0.8350
[2025-12-10 02:15:26] Epoch [18/50] Train Loss: 0.2277 | Val Loss: 0.2237 | AUC: 0.9462 | F1: 0.8000
[2025-12-10 02:15:26] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9462_lr_0.0001.pth
[2025-12-10 02:15:26] Epoch [19/50] Train Loss: 0.2283 | Val Loss: 0.2226 | AUC: 0.9470 | F1: 0.8241
[2025-12-10 02:15:26] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9470_lr_0.0001.pth
[2025-12-10 02:15:26] Epoch [20/50] Train Loss: 0.2287 | Val Loss: 0.2153 | AUC: 0.9384 | F1: 0.8529
[2025-12-10 02:15:26] Epoch [21/50] Train Loss: 0.2282 | Val Loss: 0.2160 | AUC: 0.9417 | F1: 0.8000
[2025-12-10 02:15:26] Epoch [22/50] Train Loss: 0.2212 | Val Loss: 0.2112 | AUC: 0.9452 | F1: 0.8000
[2025-12-10 02:15:26] Epoch [23/50] Train Loss: 0.2130 | Val Loss: 0.2107 | AUC: 0.9460 | F1: 0.8241
[2025-12-10 02:15:26] Epoch [24/50] Train Loss: 0.2088 | Val Loss: 0.2061 | AUC: 0.9285 | F1: 0.8462
[2025-12-10 02:15:26] Epoch [25/50] Train Loss: 0.2187 | Val Loss: 0.2038 | AUC: 0.9397 | F1: 0.8473
[2025-12-10 02:15:26] Epoch [26/50] Train Loss: 0.2155 | Val Loss: 0.2040 | AUC: 0.9493 | F1: 0.8241
[2025-12-10 02:15:27] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9493_lr_0.0001.pth
[2025-12-10 02:15:27] Epoch [27/50] Train Loss: 0.2182 | Val Loss: 0.1995 | AUC: 0.9465 | F1: 0.8000
[2025-12-10 02:15:27] Epoch [28/50] Train Loss: 0.2170 | Val Loss: 0.1948 | AUC: 0.9488 | F1: 0.8241
[2025-12-10 02:15:27] Epoch [29/50] Train Loss: 0.2225 | Val Loss: 0.1952 | AUC: 0.9437 | F1: 0.8529
[2025-12-10 02:15:27] Epoch [30/50] Train Loss: 0.2162 | Val Loss: 0.1922 | AUC: 0.9402 | F1: 0.8750
[2025-12-10 02:15:27] Epoch [31/50] Train Loss: 0.2086 | Val Loss: 0.1920 | AUC: 0.9452 | F1: 0.8182
[2025-12-10 02:15:27] Epoch [32/50] Train Loss: 0.2030 | Val Loss: 0.1899 | AUC: 0.9336 | F1: 0.8679
[2025-12-10 02:15:27] Epoch [33/50] Train Loss: 0.2065 | Val Loss: 0.1871 | AUC: 0.9445 | F1: 0.8529
[2025-12-10 02:15:27] Epoch [34/50] Train Loss: 0.2073 | Val Loss: 0.1845 | AUC: 0.9447 | F1: 0.8529
[2025-12-10 02:15:27] Epoch [35/50] Train Loss: 0.2014 | Val Loss: 0.1861 | AUC: 0.9427 | F1: 0.8585
[2025-12-10 02:15:27] Epoch [36/50] Train Loss: 0.1975 | Val Loss: 0.1826 | AUC: 0.9457 | F1: 0.8473
[2025-12-10 02:15:27] Epoch [37/50] Train Loss: 0.2097 | Val Loss: 0.1862 | AUC: 0.9386 | F1: 0.8416
[2025-12-10 02:15:27] Epoch [38/50] Train Loss: 0.1984 | Val Loss: 0.1813 | AUC: 0.9427 | F1: 0.8750
[2025-12-10 02:15:27] Epoch [39/50] Train Loss: 0.2080 | Val Loss: 0.1814 | AUC: 0.9498 | F1: 0.8241
[2025-12-10 02:15:27] Best model saved to checkpoints/pubmedbert_weighted_h256_best_AUC_0.9498_lr_0.0001.pth
[2025-12-10 02:15:27] Epoch [40/50] Train Loss: 0.1960 | Val Loss: 0.1770 | AUC: 0.9478 | F1: 0.8585
[2025-12-10 02:15:27] Epoch [41/50] Train Loss: 0.2070 | Val Loss: 0.1773 | AUC: 0.9495 | F1: 0.8358
[2025-12-10 02:15:28] Epoch [42/50] Train Loss: 0.2028 | Val Loss: 0.1750 | AUC: 0.9468 | F1: 0.8529
[2025-12-10 02:15:28] Epoch [43/50] Train Loss: 0.1971 | Val Loss: 0.1759 | AUC: 0.9435 | F1: 0.8585
[2025-12-10 02:15:28] Epoch [44/50] Train Loss: 0.2023 | Val Loss: 0.1799 | AUC: 0.9457 | F1: 0.8241
[2025-12-10 02:15:28] Epoch [45/50] Train Loss: 0.2033 | Val Loss: 0.1709 | AUC: 0.9480 | F1: 0.8696
[2025-12-10 02:15:28] Epoch [46/50] Train Loss: 0.1973 | Val Loss: 0.1797 | AUC: 0.9384 | F1: 0.8416
[2025-12-10 02:15:28] Epoch [47/50] Train Loss: 0.1952 | Val Loss: 0.1693 | AUC: 0.9485 | F1: 0.8641
[2025-12-10 02:15:28] Epoch [48/50] Train Loss: 0.2005 | Val Loss: 0.1725 | AUC: 0.9495 | F1: 0.8416
[2025-12-10 02:15:28] Epoch [49/50] Train Loss: 0.1993 | Val Loss: 0.1696 | AUC: 0.9455 | F1: 0.8696
[2025-12-10 02:15:28] Epoch [50/50] Train Loss: 0.1997 | Val Loss: 0.1688 | AUC: 0.9440 | F1: 0.8857
[2025-12-10 02:15:28] Training Finished. Running Test on Best Model...
[2025-12-10 02:15:28] Test Results - AUC: 0.9026 | F1: 0.8416
